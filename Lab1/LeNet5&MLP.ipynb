{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlynmEmKpO7YzFacZa01m4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TuxTu/HW_Arch_of_DL/blob/main/Lab1/LeNet5%26MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "pglpuTNPb5tf",
        "outputId": "2e589a40-fb00-4b6f-c7ff-594da12ec8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state loaded successfully.\n",
            "Finished Training\n",
            "Accuracy for class: 0     is 98.6 %\n",
            "Accuracy for class: 1     is 99.6 %\n",
            "Accuracy for class: 2     is 99.3 %\n",
            "Accuracy for class: 3     is 99.0 %\n",
            "Accuracy for class: 4     is 99.3 %\n",
            "Accuracy for class: 5     is 98.8 %\n",
            "Accuracy for class: 6     is 99.1 %\n",
            "Accuracy for class: 7     is 99.0 %\n",
            "Accuracy for class: 8     is 96.9 %\n",
            "Accuracy for class: 9     is 97.5 %\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         0.14%       8.000us        90.43%       5.132ms       2.566ms       0.000us         0.00%      60.000us      30.000us           0 b           0 b      70.00 Kb           0 b             2  \n",
            "                                      aten::convolution         0.90%      51.000us        90.29%       5.124ms       2.562ms       0.000us         0.00%      60.000us      30.000us           0 b           0 b      70.00 Kb           0 b             2  \n",
            "                                     aten::_convolution         0.72%      41.000us        89.39%       5.073ms       2.537ms       0.000us         0.00%      60.000us      30.000us           0 b           0 b      70.00 Kb           0 b             2  \n",
            "                                aten::cudnn_convolution        42.96%       2.438ms        76.26%       4.328ms       2.164ms      50.000us        45.05%      50.000us      25.000us           0 b           0 b      70.00 Kb      70.00 Kb             2  \n",
            "                                  cudaStreamIsCapturing         0.05%       3.000us         0.05%       3.000us       1.500us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
            "                                  cudaStreamGetPriority         0.02%       1.000us         0.02%       1.000us       0.500us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.02%       1.000us         0.02%       1.000us       0.500us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
            "                                       cudaLaunchKernel        35.12%       1.993ms        35.12%       1.993ms     142.357us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            14  \n",
            "void cask_cudnn::computeOffsetsKernel<false, false>(...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         3.60%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
            "                                        cudaMemsetAsync         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 5.675ms\n",
            "Self CUDA time total: 111.000us\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ce0b6717ebd7>\u001b[0m in \u001b[0;36m<cell line: 304>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp -r \"./model_mnist\" \"/content/drive/MyDrive\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp -r \"./records_mnist\" \"/content/drive/MyDrive\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, n_conv, n_fc, conv_ch, filter_size, fc_size, pooling_size, input_size, input_channels, n_classes, activation_fn):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        fc_input_size = input_size\n",
        "\n",
        "        self.pooling_size = pooling_size\n",
        "        if(pooling_size > 1):\n",
        "            self.has_pooling = True\n",
        "        else:\n",
        "            self.has_pooling = False\n",
        "\n",
        "        for i in range(n_conv):\n",
        "            if(i == 0):\n",
        "                input_channel = input_channels\n",
        "            else:\n",
        "                input_channel = conv_ch[i-1]\n",
        "            conv_layer = nn.Conv2d(input_channel, conv_ch[i], filter_size[i])\n",
        "            self.conv_layers.append(conv_layer)\n",
        "            fc_input_size = fc_input_size - filter_size[i] + 1\n",
        "            if(self.has_pooling):\n",
        "                fc_input_size = (fc_input_size // pooling_size) if (fc_input_size % pooling_size == 0) else (fc_input_size // pooling_size + 1)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        fc_input_size = conv_ch[-1] * fc_input_size * fc_input_size\n",
        "        self.fc_layers.append(nn.Linear(fc_input_size, fc_size[0]))\n",
        "        for i in range(1, n_fc-1):\n",
        "            fc_layer = nn.Linear(fc_size[i-1], fc_size[i])\n",
        "            self.fc_layers.append(fc_layer)\n",
        "\n",
        "        self.output_layer = nn.Linear(fc_size[-1], n_classes)\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv in self.conv_layers:\n",
        "            x = self.activation_fn(conv(x))\n",
        "            # print(f\"After conv the shape of x is: {x.shape}\")\n",
        "            if(self.has_pooling):\n",
        "                pooling = nn.MaxPool2d(self.pooling_size, self.pooling_size, padding=x.shape[-1]%self.pooling_size)\n",
        "                x = pooling(x)\n",
        "            # print(f\"After pooling the shape of x is: {x.shape}\")\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # print(f\"After flatten the shape of x is: {x.shape}\")\n",
        "\n",
        "        for fc in self.fc_layers:\n",
        "            x = self.activation_fn(fc(x))\n",
        "\n",
        "        return self.output_layer(x)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_hidden_layers, hidden_neurons, input_size, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for i in range(n_hidden_layers+1):\n",
        "            input_neurons = input_size if i == 0 else hidden_neurons[i-1]\n",
        "            output_neurons = n_classes if i == n_hidden_layers else hidden_neurons[i]\n",
        "            fc_layer = nn.Linear(input_neurons, output_neurons)\n",
        "            self.fc_layers.append(fc_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        for i, fc_layer in enumerate(self.fc_layers):\n",
        "            x = fc_layer(x)\n",
        "\n",
        "            if i < len(self.fc_layers) - 1:\n",
        "                x = F.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_lenet(model_params, model_name, device, epochs):\n",
        "    model_path = MODEL_PATH + model_name + '.pth'\n",
        "    net = LeNet(**model_params)\n",
        "\n",
        "    # load model state\n",
        "    try:\n",
        "        net.load_state_dict(torch.load(model_path))\n",
        "        print(\"Model state loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "        print(f\"No saved model state found at '{model_path}'.\")\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    # load record\n",
        "    record_path = RECORD_PATH + model_name + '.json'\n",
        "    try:\n",
        "        with open(record_path, 'r') as file:\n",
        "            record = json.load(file)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        os.makedirs(RECORD_PATH, exist_ok=True)\n",
        "        record = {\"name\": model_name, \"epochs\": 0, \"training_records\": []}\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # save current epochs\n",
        "    record[\"epochs\"] = record[\"epochs\"] + epochs\n",
        "\n",
        "    # save model\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "    test_result = {}\n",
        "\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "        test_result[classname] = accuracy\n",
        "\n",
        "    record[\"training_records\"].append({\"training_epoch\": record[\"epochs\"], \"accuracy\": test_result})\n",
        "\n",
        "    with open(record_path, 'w+') as file:\n",
        "        json.dump(record, file, indent=4)\n",
        "\n",
        "    return net\n",
        "\n",
        "def train_mlp(model_params, model_name, device, epochs):\n",
        "    model_path = MODEL_PATH + model_name + '.pth'\n",
        "    net = MLP(**model_params)\n",
        "\n",
        "    # load model state\n",
        "    try:\n",
        "        net.load_state_dict(torch.load(model_path))\n",
        "        print(\"Model state loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "        print(f\"No saved model state found at '{model_path}'.\")\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    # load record\n",
        "    record_path = RECORD_PATH + model_name + '.json'\n",
        "    try:\n",
        "        with open(record_path, 'r') as file:\n",
        "            record = json.load(file)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        os.makedirs(RECORD_PATH, exist_ok=True)\n",
        "        record = {\"name\": model_name, \"epochs\": 0, \"training_records\": []}\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # save current epochs\n",
        "    record[\"epochs\"] = record[\"epochs\"] + epochs\n",
        "\n",
        "    # save model\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "    test_result = {}\n",
        "\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "        test_result[classname] = accuracy\n",
        "\n",
        "    record[\"training_records\"].append({\"training_epoch\": record[\"epochs\"], \"accuracy\": test_result})\n",
        "\n",
        "    with open(record_path, 'w+') as file:\n",
        "        json.dump(record, file, indent=4)\n",
        "\n",
        "    return MLP\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = './model_mnist/'\n",
        "    RECORD_PATH = './records_mnist/'\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    batch_size = 4\n",
        "\n",
        "    classes = (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\")\n",
        "\n",
        "    # Load dataset\n",
        "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Load testset\n",
        "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    params_mlp1 = {\"n_hidden_layers\": 1, \"hidden_neurons\": [], \"input_size\": trainset[0][0].shape[-1]**2, \"n_classes\": 10}\n",
        "\n",
        "    params_mlp2 = {\"n_hidden_layers\": 2, \"hidden_neurons\": [300, 100], \"input_size\": trainset[0][0].shape[-1]**2, \"n_classes\": 10}\n",
        "\n",
        "    name_mlp2 = \"MLP2_\"+str(params_mlp2[\"hidden_neurons\"][0])+str(params_mlp2[\"hidden_neurons\"][1])\n",
        "\n",
        "    params_lenet5 = {'n_conv': 2, 'n_fc': 3, 'conv_ch': [6, 16], 'filter_size': [5, 5], 'fc_size': [120, 84], 'pooling_size': 2, 'input_size': trainset[0][0].shape[-1], 'input_channels': 1, 'n_classes': len(classes), 'activation_fn': F.relu}\n",
        "\n",
        "    name_lenet5 = \"LeNet-5\"\n",
        "\n",
        "    for neurons in range(30, 301, 30):\n",
        "        params_mlp1[\"hidden_neurons\"] = [neurons]\n",
        "        name_mlp1 = \"MLP1\" + str(neurons)\n",
        "        train_mlp(params_mlp1, name, device, 20)\n",
        "\n",
        "    train_mlp(params_mlp2, name_mlp2, device, 100)\n",
        "\n",
        "    lenet5_model = train_lenet(params_lenet5, name_lenet5, device, 0)\n",
        "\n",
        "    data = list(testloader)[0]\n",
        "    inputs = data[0].to(device)\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "        profile_memory=True\n",
        "    ) as prof:\n",
        "        lenet5_model(inputs)\n",
        "\n",
        "    print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"./model_mnist\" \"/content/drive/MyDrive\"\n",
        "!cp -r \"./records_mnist\" \"/content/drive/MyDrive\""
      ]
    }
  ]
}