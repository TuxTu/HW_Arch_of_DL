{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOuBkSrc7ZXFszwRL7vzlHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TuxTu/HW_Arch_of_DL/blob/main/LeNet5%26MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pglpuTNPb5tf",
        "outputId": "46638633-a4fd-4349-bc1e-9fedfa6c3778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved model state found at './model_mnist/LeNet-5.pth'.\n",
            "[1,  2000] loss: 1.398\n",
            "[1,  4000] loss: 0.287\n",
            "[1,  6000] loss: 0.202\n",
            "[1,  8000] loss: 0.149\n",
            "[1, 10000] loss: 0.124\n",
            "[1, 12000] loss: 0.119\n",
            "[1, 14000] loss: 0.103\n",
            "[2,  2000] loss: 0.083\n",
            "[2,  4000] loss: 0.074\n",
            "[2,  6000] loss: 0.087\n",
            "[2,  8000] loss: 0.075\n",
            "[2, 10000] loss: 0.069\n",
            "[2, 12000] loss: 0.065\n",
            "[2, 14000] loss: 0.065\n",
            "Finished Training\n",
            "Accuracy for class: 0     is 99.5 %\n",
            "Accuracy for class: 1     is 99.3 %\n",
            "Accuracy for class: 2     is 98.6 %\n",
            "Accuracy for class: 3     is 97.8 %\n",
            "Accuracy for class: 4     is 99.7 %\n",
            "Accuracy for class: 5     is 98.9 %\n",
            "Accuracy for class: 6     is 98.2 %\n",
            "Accuracy for class: 7     is 98.2 %\n",
            "Accuracy for class: 8     is 98.3 %\n",
            "Accuracy for class: 9     is 94.6 %\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, n_conv, n_fc, conv_ch, filter_size, fc_size, pooling_size, input_size, input_channels, n_classes, activation_fn):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        fc_input_size = input_size\n",
        "\n",
        "        self.pooling_size = pooling_size\n",
        "        if(pooling_size > 1):\n",
        "            self.has_pooling = True\n",
        "        else:\n",
        "            self.has_pooling = False\n",
        "\n",
        "        for i in range(n_conv):\n",
        "            if(i == 0):\n",
        "                input_channel = input_channels\n",
        "            else:\n",
        "                input_channel = conv_ch[i-1]\n",
        "            conv_layer = nn.Conv2d(input_channel, conv_ch[i], filter_size[i])\n",
        "            self.conv_layers.append(conv_layer)\n",
        "            fc_input_size = fc_input_size - filter_size[i] + 1\n",
        "            if(self.has_pooling):\n",
        "                fc_input_size = (fc_input_size // pooling_size) if (fc_input_size % pooling_size == 0) else (fc_input_size // pooling_size + 1)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        fc_input_size = conv_ch[-1] * fc_input_size * fc_input_size\n",
        "        self.fc_layers.append(nn.Linear(fc_input_size, fc_size[0]))\n",
        "        for i in range(1, n_fc-1):\n",
        "            fc_layer = nn.Linear(fc_size[i-1], fc_size[i])\n",
        "            self.fc_layers.append(fc_layer)\n",
        "\n",
        "        self.output_layer = nn.Linear(fc_size[-1], n_classes)\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv in self.conv_layers:\n",
        "            x = self.activation_fn(conv(x))\n",
        "            # print(f\"After conv the shape of x is: {x.shape}\")\n",
        "            if(self.has_pooling):\n",
        "                pooling = nn.MaxPool2d(self.pooling_size, self.pooling_size, padding=x.shape[-1]%self.pooling_size)\n",
        "                x = pooling(x)\n",
        "            # print(f\"After pooling the shape of x is: {x.shape}\")\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # print(f\"After flatten the shape of x is: {x.shape}\")\n",
        "\n",
        "        for fc in self.fc_layers:\n",
        "            x = self.activation_fn(fc(x))\n",
        "\n",
        "        return self.output_layer(x)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_hidden_layers, hidden_neurons, input_size, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for i in range(n_hidden_layers+1):\n",
        "            input_neurons = input_size if i == 0 else hidden_neurons[i-1]\n",
        "            output_neurons = n_classes if i == n_hidden_layers else hidden_neurons[i]\n",
        "            fc_layer = nn.Linear(input_neurons, output_neurons)\n",
        "            self.fc_layers.append(fc_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        for i, fc_layer in enumerate(self.fc_layers):\n",
        "            x = fc_layer(x)\n",
        "\n",
        "            if i < len(self.fc_layers) - 1:\n",
        "                x = F.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_lenet(model_params, model_name, device, epochs):\n",
        "    model_path = MODEL_PATH + model_name + '.pth'\n",
        "    net = LeNet(**model_params)\n",
        "\n",
        "    # load model state\n",
        "    try:\n",
        "        net.load_state_dict(torch.load(model_path))\n",
        "        print(\"Model state loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "        print(f\"No saved model state found at '{model_path}'.\")\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    # load record\n",
        "    record_path = RECORD_PATH + model_name + '.json'\n",
        "    try:\n",
        "        with open(record_path, 'r') as file:\n",
        "            record = json.load(file)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        os.makedirs(RECORD_PATH, exist_ok=True)\n",
        "        record = {\"name\": model_name, \"epochs\": 0, \"training_records\": []}\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # save current epochs\n",
        "    record[\"epochs\"] = record[\"epochs\"] + epochs\n",
        "\n",
        "    # save model\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "    test_result = {}\n",
        "\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "        test_result[classname] = accuracy\n",
        "\n",
        "    record[\"training_records\"].append({\"training_epoch\": record[\"epochs\"], \"accuracy\": test_result})\n",
        "\n",
        "    with open(record_path, 'w+') as file:\n",
        "        json.dump(record, file, indent=4)\n",
        "\n",
        "def train_mlp(model_params, model_name, device, epochs):\n",
        "    model_path = MODEL_PATH + model_name + '.pth'\n",
        "    net = MLP(**model_params)\n",
        "\n",
        "    # load model state\n",
        "    try:\n",
        "        net.load_state_dict(torch.load(model_path))\n",
        "        print(\"Model state loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "        print(f\"No saved model state found at '{model_path}'.\")\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    # load record\n",
        "    record_path = RECORD_PATH + model_name + '.json'\n",
        "    try:\n",
        "        with open(record_path, 'r') as file:\n",
        "            record = json.load(file)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        os.makedirs(RECORD_PATH, exist_ok=True)\n",
        "        record = {\"name\": model_name, \"epochs\": 0, \"training_records\": []}\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # save current epochs\n",
        "    record[\"epochs\"] = record[\"epochs\"] + epochs\n",
        "\n",
        "    # save model\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "    test_result = {}\n",
        "\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "        test_result[classname] = accuracy\n",
        "\n",
        "    record[\"training_records\"].append({\"training_epoch\": record[\"epochs\"], \"accuracy\": test_result})\n",
        "\n",
        "    with open(record_path, 'w+') as file:\n",
        "        json.dump(record, file, indent=4)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = './model_mnist/'\n",
        "    RECORD_PATH = './records_mnist/'\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    batch_size = 4\n",
        "\n",
        "    classes = (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\")\n",
        "\n",
        "    # Load dataset\n",
        "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Load testset\n",
        "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    params_mlp1 = {\"n_hidden_layers\": 1, \"hidden_neurons\": [], \"input_size\": trainset[0][0].shape[-1]**2, \"n_classes\": 10}\n",
        "\n",
        "    params_mlp2 = {\"n_hidden_layers\": 2, \"hidden_neurons\": [300, 100], \"input_size\": trainset[0][0].shape[-1]**2, \"n_classes\": 10}\n",
        "\n",
        "    name_mlp2 = \"MLP2_\"+str(params_mlp2[\"hidden_neurons\"][0])+str(params_mlp2[\"hidden_neurons\"][1])\n",
        "\n",
        "    params_lenet5 = {'n_conv': 2, 'n_fc': 3, 'conv_ch': [6, 16], 'filter_size': [5, 5], 'fc_size': [120, 84], 'pooling_size': 2, 'input_size': trainset[0][0].shape[-1], 'input_channels': 1, 'n_classes': len(classes), 'activation_fn': F.relu}\n",
        "\n",
        "    name_lenet5 = \"LeNet-5\"\n",
        "\n",
        "    # for neurons in range(30, 301, 30):\n",
        "    #     params_mlp1[\"hidden_neurons\"] = [neurons]\n",
        "    #     name_mlp1 = \"MLP1\" + str(neurons)\n",
        "    #     train_mlp(params_mlp1, name, device, 20)\n",
        "    # train_mlp(params_mlp2, name_mlp2, device, 100)\n",
        "\n",
        "    train_lenet(params_lenet5, name_lenet5, device, 2)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"./model_mnist\" \"/content/drive/MyDrive\"\n",
        "!cp -r \"./records_mnist\" \"/content/drive/MyDrive\""
      ]
    }
  ]
}